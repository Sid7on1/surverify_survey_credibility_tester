{
  "agent_id": "coder2",
  "task_id": "task_5",
  "files": [
    {
      "filename": "tests/test_surverify.py",
      "purpose": "Unit tests for SurVerify algorithm correctness",
      "priority": "medium",
      "dependencies": [
        "pytest",
        "numpy",
        "surverify"
      ],
      "key_functions": [
        "test_acceptance_when_close",
        "test_rejection_when_far",
        "test_early_stopping",
        "test_sample_complexity"
      ],
      "estimated_lines": 250,
      "complexity": "medium"
    },
    {
      "filename": "tests/test_regression_models.py",
      "purpose": "Unit tests for regression model implementations",
      "priority": "medium",
      "dependencies": [
        "pytest",
        "numpy",
        "regression_models"
      ],
      "key_functions": [
        "test_lasso_fit",
        "test_ridge_fit",
        "test_kernel_fit",
        "test_bounded_constraints"
      ],
      "estimated_lines": 200,
      "complexity": "medium"
    }
  ],
  "project_info": {
    "project_name": "SurVerify_Survey_Credibility_Tester",
    "project_type": "optimization",
    "description": "A dimension-agnostic algorithm for testing the credibility of survey data in regression tasks. SurVerify determines whether a regression model trained on survey data will yield similar conclusions when applied to the true population distribution, using a novel Functional Distance of Distributions (FDD) metric. The algorithm achieves sample complexity independent of data dimension by avoiding full model reconstruction.",
    "key_algorithms": [
      "Functional_Distance_of_Distributions",
      "SurVerify_Algorithm",
      "Two_sided_Generalization_Bound",
      "Early_Stopping_Criterion",
      "Rademacher_Complexity_Bounds"
    ],
    "main_libraries": [
      "numpy",
      "scikit-learn",
      "scipy",
      "matplotlib",
      "pandas",
      "argparse",
      "logging",
      "tqdm"
    ]
  },
  "paper_content": "PDF: stat.ML_2508.20616v1_Dimension-Agnostic-Testing-of-Survey-Data-Credibil.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nDimension Agnostic Testing of Survey Data\nCredibility through the Lens of Regression\nDebabrota Basu\n\u00c9quipe Scool, Univ. Lille, Inria,\nCNRS, Centrale Lille, UMR 9189- CRIStAL\nF-59000 Lille, FranceSourav Chakraborty\nIndian Statistical Institute\nKolkata, India\nDebarshi Chanda\nIndian Statistical Institute\nKolkata, IndiaBuddha Dev Das\nIndian Statistical Institute\nKolkata, IndiaArijit Ghosh\nIndian Statistical Institute\nKolkata, India\nArnab Ray\nIndian Statistical Institute\nKolkata, India\nAbstract\nAssessing whether a sample survey credibly represents the population is a crit-\nical question for ensuring the validity of downstream research. Generally, this\nproblem reduces to estimating the distance between two high-dimensional distri-\nbutions, which typically requires a number of samples that grows exponentially\nwith the dimension. However, depending on the model used for data analysis, the\nconclusions drawn from the data may remain consistent across different underly-\ning distributions. In this context, we propose a task-based approach to assess the\ncredibility of sampled surveys. Specifically, we introduce a model-specific dis-\ntance metric to quantify this notion of credibility. We also design an algorithm to\nverify the credibility of survey data in the context of regression models. Notably,\nthe sample complexity of our algorithm is independent of the data\u2019s dimension.\nThis efficiency stems from the fact that the algorithm focuses on verifying the\ncredibility of the survey data rather than reconstructing the underlying regression\nmodel. Furthermore, we show that if one attempts to verify credibility by re-\nconstructing the regression model, the sample complexity scales linearly with the\ndimensionality of the data. We prove the theoretical correctness of our algorithm\nand numerically demonstrate our algorithm\u2019s performance.\n1 Introduction\nSocio-economic surveys are conducted globally to collect data on population characteristics for a\nvariety of purposes, including demographic and economic analyses, educational planning, poverty\nassessments, exit poll evaluations, and measuring progress toward national goals [GFJC+11,\nKKM+21]. The primary aim of many surveys is to support inference-driven analyses that uncover\npatterns to inform future research and policy decisions [HWB17, GoC24], as well as to monitor and\nevaluate the long-term impacts of various policies [BDI+20]. These survey datas serve as long-term\nbenchmarks for validating research hypotheses [SD94, GoC24]. Therefore, verifying the credibility\nof such survey data is essential to ensure the validity of downstream analyses.\nPreprint. Under review.arXiv:2508.20616v1  [cs.LG]  28 Aug 2025\n\n--- Page 2 ---\nIdeally, properly collected data should be a faithful representation of the population, and representa-\ntive data should ensure the validity of subsequent research. However, in practice, survey data rarely\nreflect the population perfectly [Mau17, IK20]. In the social sciences, it is rare to find large-scale\nsurveys that do not employ stratified or multistage sampling techniques [GFJC+11, Loh21, Kal21].\nIn practice, these surveys are often carried out under logistical constraints.\nDetermining whether a collected sample accurately represents the population is a longstanding chal-\nlenge in both statistics and computer science\u2014often framed in the latter as the problem of measuring\nthe closeness between two distributions [BFR+00, Can22]. In other words, verifying representative-\nness is inherently inefficient and resource-intensive. In many cases, data collectors do not even claim\nthat their samples are representative. Nevertheless, such data are routinely used for population-level\nresearch. Naturally, this raises the question of how much trust one can place in the resulting anal-\nyses. The answer hinges on the \u201ccredibility\u201d of the data. In this paper, we propose a principled\napproach to quantify the credibility of survey data, along with an efficient method for doing so.\nA key observation is that if the goal is merely to ensure the validity of research conducted using\nthe data, then verifying whether the data is fully representative of the population may be unneces-\nsary\u2014or even excessive. In such cases, traditional methods for assessing representativeness may be\ntoo rigid or resource-intensive to be practical. Specifically, if the analysis relies on a well-established\nclass of inference tools, we should be able to certify that any conclusions drawn using these tools\nfrom the given survey are valid, regardless of whether the data perfectly mirrors the population.\nOne widely used and interpretable method for analyzing survey data is fitting a regression model.\nFor example, [BJ08] utilizes data from the British Health and Lifestyle Survey (1984-1985) and\nits longitudinal follow-up in May 2003 to demonstrate a strong association between mortality and\nsocio-economic status. Motivated by such applications, in this paper, we ask the following question:\nCan we verify whether the conclusions drawn from a regression model fitted on a given survey\ndataset would yield similar results if applied to the entire population?\nConducting large-scale sample surveys is often complex and costly, which can result in compro-\nmised data quality. However, it is commonly assumed that collecting a small number of additional\nhigh-quality data points can help validate the overall dataset. Building on this idea, our approach\nto the question above involves leveraging a limited amount of high-quality supplementary sam-\nple\u2014alongside the original survey data\u2014to assess the credibility of the survey in the context of\nregression models. The central objective is to develop an efficient algorithm that minimizes both\ncomputational cost and sample complexity (i.e., the number of additional samples required).\nProblem Formulation. Typically, once a sampling-based study is designed, survey data is collected\nfrom an underlying population. In line with the structure of most socio-economic surveys, we as-\nsume that the survey dataset Sconsists of tabular numeric covariates and a scalar response variable.\nSpecifically, each data point in Sis of the form (x, y), where the covariates x\u2208Rdand the response\nvariable y\u2208R. Most of the time the dimension, that is d, is quite large.\nWe denote by D\u2217the distribution of the (x, y)tuples of the whole population. If the dataset S\nwas obtained after perfect sampling techniques, i.e. by drawing independent samples from an un-\nknown distribution D\u2217, then one would call the survey data Sto be a credible representation of the\npopulation. But due to various limitations, the dataset Scollected might be obtained by drawing\nsamples from some other distribution DS. So the question about how credible is Sas a represen-\ntation of the population boils down to understanding the distance between the two distributions D\u2217\nandDS. We will call D\u2217to be the true distribution andDSto be the sample distribution . Estimating\nthe distance between two high-dimensional distributions is very inefficient, and hence, impracti-\ncal [Can15, Can22]. This has motivated development of distance measures between datasets, such\nas Optimal Transport Dataset Distance [AMF20], which are costly to compute in high dimensions.\nIn particular, we list the sample complexities of some of the most well-studied distributional dis-\ntances when the distributions are defined over a d-dimensional space:\n\u2022TV: The problem of testing the TV distance of two distributions over a support of size krequire\n\u0398(k/logk)samples [CJKL22]. Given the distribution is over {0,1}d, the sample complexity\nis\u0398(2d/d). If we have a continuous distributions over [0,1]ddiscretized with bin width \u03b5, the\nsample complexity would be \u0398(\u03b5d/(dlog(1 /\u03b5))).\n2\n\n--- Page 3 ---\n\u2022Wasserstein: For two bounded-moment distributions over a d-dimensional space, the Wasserstein\ndistance requires \u2126(\u03f5\u2212d)samples for the empirical measure to converge to distance \u03f5[Lei20].\n\u2022KL: For two distributions over a d-dimensional bounded space, the minimax-optimal estimation\nof the KL divergence requires \u2126(\u03b5\u2212d)samples [ZL20].\nIn all the cases discussed above, to test closeness of distributions given sampling access to them,\nrequires the number of samples to grow exponentially with the number of dimensions. In contrast,\nthe number of samples-to-test required by our method is independent of dimension.\nSamples collected from a survey are typically used for various data interpretation and deduction\ntasks, e.g. regression, classification etc. In all these cases, one aims to find a model from a given\nmodel class, say F, that minimises a task-specific loss function. For example, for regression, we aim\nto find the regression function that minimise the square loss over the survey data. If L:R2\u2192Ris\nthe loss function, then the model learnt from the survey set Sis:\nfS\u225cargmin\nf\u2208F1\nmX\n(x,y)\u2208SL(f(x), y).\nTo validate the credibility of a survey data, we propose to test whether the model fSderived from the\nsurvey data Smatches the model f\u2217, that would have been odtained if the dataset Sbeen a credible\nrepresentation of the population D\u2217.\nf\u2217\u225cargmin\nf\u2208FE\n(x,y)\u223cD\u2217L(f(x), y).\nWe will assume that we have access to a small sample set, called the validation dataset, obtained by\ndrawing i.i.d. samples from the true distribution D\u2217.\nDepending on the problems, different metrics have been proposed to quantify the closeness of dis-\ntributions [GS02]. Our goal is to validate the quality of the survey data Sby estimating the distance\noffSfrom f\u2217. We use the distributional \u21132distance to quantify the closeness of regression models.\nDefinition 1 (Distributional \u21132-Distance between Functions) .Letfandgbe real-valued functions\nonRd, andDbe a distribution on Rd. The distributional \u21132-distance between fandgonDis:\ndist D(f, g)\u225cq\nE\nx\u223cD[(f(x)\u2212g(x))2]\nThus, our problem can be formulated as follows: Given a survey set S(drawn according to some\nunknown distribution DS) and a model class F, we aim to sample a small number of new data\npoints from the true distribution D\u2217and determine whether dist D\u2217(fS, f\u2217)lies within a specified\nacceptable threshold. Ideally, the number of new samples drawn from D\u2217should be very small and\nindependent of the dimensionality of the ambient space.\nRelated Works. Our work lies at the intersection of distribution testing andmodel validation . Dis-\ntribution identity testing\u2014determining whether an unknown distribution matches a known one\u2014has\nbeen widely studied [BFR+00, Pan08, VV17, DGPP18], with comprehensive surveys summariz-\ning key results [Can22, Can15]. Recent efforts have focused on high-dimensional settings, where\ntesting structured distributions such as Ising models or Bayesian networks poses significant chal-\nlenges [DP17, DDK18, CDKS17, BGMV20, BGKV21]. However, these approaches often suffer\nfrom exponential sample complexity in the dimension d[BBC+20, BCvV21]. In contrast, model\nvalidation has long been studied through statistical tests for evaluating model fit, especially in re-\ngression and parametric models [Sne77, PC84, DM98, SZ21, Stu97]. These approaches often rely\non strong assumptions about the model or the data. Our work brings these two perspectives together\naiming to develop scalable and principled methods for validating the credibility of high dimensional\nsurveys through the lens of regression models.\nOur Results. In this work, we consider the class of regression models for the model-specific testing\nproblem. We consider two common assumptions of regression models for our scenario \u2013 exogenous\nnoise in observation [RL03, MRT18] and boundedness of involved variables and the model [MRT18,\nJWHT21].1Exogeneity of noise ensures exact identifiability of the underlying model, i.e. we do\n1Note that these two assumptions are not absolutely necessary for the proposed framework to function but\nto provide clean and rigorous theoretical analysis. We discuss further in Section 6.\n3\n\n--- Page 4 ---\nnot have unidentified covariates that influence the outcome. Boundedness is usually satisfied in our\nsetting as the survey datasets always have finite entries and can be normalized.\nAssumption 1 (Exogenous Noise ).For a regression model y=f(x) +\u03b7, we have:\n(a)Homoskedasticity: The noise \u03b7has constant variance, i.e. Var[\u03b7|x] =\u03c32\n\u03b7,\n(b)Non-correlation: The noise \u03b7is uncorrelated with x\u2208Rdand independent across observations.\nAssumption 2 (Boundedness ).We assume that the response variable satisfy |y| \u22641, the covariates\nsatisfy \u2225x\u2225\u221e\u22641, and f(x)\u22641.\nGiven this context, we elaborate the main contributions of this paper:\n1.Task-Specific Credibility Testing: We propose the framework of task-specific credibility testing\nof survey that checks whether it leads to valid inference while used with ML models. Specifically,\nwe focus on regression models \u2013 linear with \u21131and\u21132regularizers, and kernel with \u21132regularizers.\nThis is a deviation from the classic distribution testing frameworks that check for some divergence\n(e.g. TV , KL, Wasserstein) between two data distributions. But these frameworks require expo-\nnential number of samples with respect to the dimension of data. This is infeasible for a survey\nsetting. Thus, we propose a new data-distribution specific metric, called the Functional Distance\nof Distributions ( FDD), between two regression model, and leverage it to test closeness of two data\ndistributions through the lens of regression.\n2. Generic Algorithm for Model-Specific Testing for Regression Models: We propose\nSurVerify to test whether a regression model learned from a given survey data Sis close to a\nmodel learned using independent and identically distributed ( i.i.d.) samples collected from an under-\nlying distribution. SurVerify does this by checking whether the loss of the survey-based model\nand the i.i.d. model match up to pre-computed threshold. We prove that SurVerify is correct\nwith high probability up to a user-defined tolerance gap. We show that the worst-case sample com-\nplexity2ofSurVerify to conduct a correct test is independent of the dimension and fixed across\nregression models. Additionally, if the model is very far in the FDD metric, SurVerify detects it\nearlier with less samples. Finally, we numerically verify the correctness and sample complexity of\nSurVerify across datasets.\nTo conduct our theoretical analysis, we propose a new two-sided bound on generalization error of a\nregression model, which is of independent interest for statistical learning.\nOrganization of the paper: Section 2 introduces the preliminaries. Section 3 discusses the new\nmetric. Section 4 presents our main algorithm, SurVerify , with theoretical guarantees. Proofs\nappear in the Appendix. Section 5 reports experimental results.\n2 Preliminaries: Regression Models and Rademacher Complexity\nThe survey set is denoted as S, and its size as m=|S|. We denote X, andYto be the input\nand output spaces, respectively. Fdenotes a hypothesis sets consisting of hypothesis f:X \u2192 Y .\nSimilarly, Fdenotes the set of regression functions f:X \u2192 Y , and the coefficient associated with\nthe regression functions are denoted \u03b8.\u27e8\u00b7,\u00b7\u27e9denotes inner product, and \u2225\u00b7\u2225pdenotes the \u2113pnorm.\nA Primer on Regression: Linear and Kernel. Performing regression on survey data to fit rea-\nsonable models over the population is central to a wide variety of analysis tasks [CGG+15, Pan17,\nMS17]. Often, the observations collected to construct a survey dataset are the result of a complex\nsampling design reflecting the need to collect data as efficiently as possible within cost constraints.\nBroadly, the problem of regression is as follows: given an input space X \u2286Rd, an output range\nY \u2286R, a distribution over X \u00d7Y , a hypothesis set F:X \u2192 Y , and a loss function L:Y \u00d7 Y \u2192 R,\noutput a hypothesis h\u2208 F that minimizes loss w.r.t. the distribution over X \u00d7Y . Here, we consider\nthe regression model with additive noise \u03b7. That is, y=f(x) +\u03b7.\nIn this work, we consider three widely used hypothesis classes for the regression problem. First, we\nconsider linear regression that tries to fit a linear model between the response and the covariates, i.e.\ny=\u27e8\u03b8\u2217,x\u27e9+\u03b7,where \u03b8\u2217,x\u2208Rd, y, \u03b7\u2208R\n2Sample complexity is the number of sample-to-test theSurVerify needs from true distribution D\u2217.\n4\n\n--- Page 5 ---\nWe consider both the cases of \u21131and\u21132-norm bounded coefficients for the linear regression model,\nknown as Lasso andRidge regression respectively. These are also called the bounded weight\nhypothesis classes Fp={x\u2192 \u27e8\u03b8,x\u27e9:\u2225\u03b8\u2225p\u22641}.Henceforward, we use these two terms\ninterchangeably. We denote the hypothesis sets containing \u21131and\u21132bounded linear regressions as\nF1, andF2, respectively.\nWe also consider the Kernel Regression model, where we associate with the input space Xa PDS\n(Positive Semidefinite Symmetric) kernel K:X \u00d7 X \u2192 Rthat implicitly defines an associated\nfunction \u03a6:X \u2192 Hsuch that: K(x,x\u2032) =\u27e8\u03a6(x),\u03a6(x\u2032)\u27e9. The regression model is a linear\nmodel on this Hilbert space Hwith the underlying coefficients \u03b8\u2217\u2208H, and the model is:\ny=\u27e8\u03b8\u2217,\u03a6(x)\u27e9+\u03b7where \u03b8\u2217,\u03a6(x)\u2208H, y, \u03b7\u2208R\nIn this case, we consider the hypothesis class consisting of coefficients \u03b8with bounded H-norm.\nWe denote the hypothesis classes containing the kernel as FK. For all the regression models, we\nconsider the loss function Lto be the squared error loss function defined as L(y, y\u2032)\u225c(y\u2212y\u2032)2.\nRademacher Complexity. The Rademacher complexity of a function class Gplays a crucial role\nin the generalization bounds for several learning models [MRT18], and also in our analysis. The\nempirical Rademacher complexity is measured w.r.t. a particular set of samples S.\nDefinition 2 (Empirical Rademacher Complexity ).Given a family of functions Gcontaining\nfunctions g:Z \u2192 [0, M]andS= (z1, . . . , z m)a fixed sample with elements in Z. Then, the\nempirical Rademacher complexity b\u211cS(G)ofGw.r.t. Sis\nb\u211cS(G)\u225cE\nr\"\nsup\ng\u2208G1\nmX\ni\u2208mrig(zi)#\n,\nwhere ri\u2019s are i.i.d Rademacher random variables taking value uniformly in {\u22121,+1}.\n3 Functional Distance of Distributions ( FDD): A Novel Metric\nWe define the model-specific distance between distributions that quantifies the distance between\ndistributions w.r.t. a model class Fand a true distribution D\u2217.\nDefinition 3 (FDDF\nD\u2217(D1,D2)).Given a true distribution D\u2217, a model class F, and an associated\nloss function LF, letfD1, and fD2be the optimal models in FforD1, andD2, respectively. We\ndefine the model-specific distance w.r.t. the true distribution D\u2217as:\nFDDF\nD\u2217(D1,D2) =dist D\u2217(fD1, fD2).\nGiven a true distribution D\u2217, the model specific testing transforms the problem of testing close-\nness of distributions to testing closeness of functions over a given true distribution. Given a hy-\npothesis set F, and a loss function L, it associates with each distribution Da function fDas\nfD= argminf\u2208FEx,y\u223cDL(f(x), y).\nConsequently, given a set of distributions D, we can define the set of hypotheses associated with\nthem as FD={fD\u2208 F | D \u2208 D}.\nBy a standard fact of Lpspaces [SS12], if the functions f\u2208 FDhas bounded second moment w.r.t.\nD\u2217, i.e.E(x,y)\u223cD\u2217\u0002f2(x)\u0003<\u221e, then the set\u0000FD,dist D\u2217\u0001constitutes a L2-space. If we consider\nthe equivalence relation f1\u223cf2, i.e., if and if dist D\u2217(f1, f2) = 0 ,dist D\u2217defines a metric on\nthe resulting partition. Correspondingly, FDDF\nD\u2217induces a metric on the partition of distributions D\ninduced by the equivalence relation D1\u223c D 2if and only if dist D\u2217(fD1, fD2) = 0 .\nIt is important to note that the FDD metric can be zero even when the distributions D1andD2differ\nsignificantly. Therefore, when the goal is to assess whether two distributions are equivalent with\nrespect to a specific task, FDD serves as an appropriate measure. For regression models that satisfy\nthe exogenous noise assumption (Assumption 1) under the squared loss, we establish the following\nrelationship between the loss and FDD.\nLemma 4 (FDD-variance Decomposition of Loss) .If the model class Fsatisfies Assumption 1, then\nE\n(x,y)\u223cD\u2217\u00ee\n(fS(x)\u2212y)2\u00f3\n= (FDDF\nD\u2217(DS,D\u2217))2+\u03c32\n\u03b7.\n5\n\n--- Page 6 ---\nThe above lemma can be intuitively viewed as a decomposition result, akin to the classic bias-\nvariance breakdown of estimation error [Was04]. It states that the expected loss of a model learned\nfrom the survey, evaluated with respect to the true distribution, can be decomposed into two com-\nponents: the approximation error (i.e., how far the learned model is from the optimal one) and the\nintrinsic noise (i.e., the error incurred even by the best possible model).\n4SurVerify : Testing Credibility with Regression and Fixed Confidence\nWe first describe the algorithm design and then establish its efficiency in terms of sample complexity.\nIn order to prove this result, we propose a two-sided generalization bound for regression and also a\nlower bound on methods reconstructing complete model to test dataset distances.\n4.1 Dimension Agnostic Algorithm Design with Early Stopping\nWe now present our algorithmic framework, SurVerify , which verifies whether a regression\nmodel learned from a survey sample Sis close to the true optimal model in \u21132-distance (Defini-\ntion 1). The algorithm performs this testing using a small number of samples drawn from the true\ndistribution D\u2217. We refer to them as sample-to-test .\nAlgorithm 1 SurVerify (S\u2282R(d+1),D\u2217, \u03f5, \u03b4,F)\nRequire: |S| \u2265 M (F)\n1:Initialize m\u2190 |S|,SD\u2217\u2190 \u2205, \u03c4\u2190 \u23082\n(1.9\u03f5)2log\u00003\n\u03b4\u0001\u2309\n2:fS\u2190argminf\u2208F1\nmP\n(x,y)\u2208S(f(x)\u2212y)2\n3:\u02c6LS\u21901\nmP\n(x,y)\u2208S(fS(x)\u2212y)2\n4:\u02c6\u03b3\u21900, t\u21900\n5:while t < \u03c4 do\n6: SD\u2217\u2190SD\u2217\u222a {(xi, yi)}, where (xi, yi)\u223c D\u2217\n7: \u02c6\u03b3\u2190\u02c6\u03b3+ (fS(xi)\u2212yi)2\n8: if\u02c6\u03b3\u2212t\u02c6LS>1.1t\u03f5+\u00bb\n2tlog\u00003\u03c4\n\u03b4\u0001then\n9: return REJECT\n10: t\u2190t+ 1\n11:if\u02c6\u03b3\u2212\u03c4\u02c6LS\u22643\u03c4\u03f5then\n12: return ACCEPT\n13:else\n14: return REJECT\nWe begin with an overview of our algorithmic framework, SurVerify , before presenting its for-\nmal correctness guarantee. The core idea behind SurVerify is to assess the credibility of a survey\nsample Sthrough a two-phase procedure. In the first phase (Lines 2 and 3), the algorithm fits a\nregression model fSusing the survey data. In the second phase (Lines 5 to 14), it evaluates the\nreliability of fSby estimating its expected loss under the true distribution D\u2217, using a small number\nof i.i.d. samples-to-test. Specifically, it computes an additive estimate \u02c6\u03b3of the expected loss of fS\non data from D\u2217. The algorithm then compares \u02c6\u03b3against a fixed threshold: if the estimated loss is\nlow enough (Line 11 and onward), it outputs ACCEPT; otherwise, it outputs REJECT.\nTo be more sample-efficient, SurVerify also incorporates an early rejection criterion (Line 8) to\nterminate the evaluation of fSquickly when it incurs a large loss on the sample-to-test, i.e., when\nthe loss is deviating enough to be detected with only a few samples. Notably, the total number\nof samples-to-test required from D\u2217, denoted \u03c4, isO\u00001\n\u03f52log\u00001\n\u03b4\u0001\u0001, and is independent of the data\ndimension. This sample efficiency makes SurVerify well-suited for high-dimensional settings\nwhere direct access to the true distribution is limited, and also in the settings where collecting sam-\nples is costly (e.g. medical data).\n4.2 Theoretical Analysis: Correctness, Sample Complexity, and Sufficient Size of Survey\nThe following theorem is the main structural result of this work. It shows that the validity of a model\nlearned from survey data can be efficiently certified using only a small number of i.i.d. samples-to-\n6\n\n--- Page 7 ---\ntest from the true distribution. This is especially useful when survey data is abundant but access to\nthe true distribution is limited (e.g. medical data, socioeconomic data). By leveraging the framework\nof functional distance of distributions (defined in Section 3), SurVerify reliably distinguishes\nbetween two datasets with high confidence and low sample complexity.\nTheorem 5 (Correctness of SurVerify and Sample Complexity ).Given a survey sample S\n(drawn from an unknown distribution DS), a model class Fand i.i.d. sampling access to the true\ndistribution D\u2217then for any \u03f5and\u03b4\u2208(0,1), if the size of Sis large enough (Table 1) then\n1. If(FDDF\nD\u2217(DS,D\u2217))2\u2264\u03f5, then SurVerify outputs ACCEPT with probability 1\u2212\u03b4.\n2. If(FDDF\nD\u2217(DS,D\u2217))2>5\u03f5, then SurVerify outputs REJECT with probability 1\u2212\u03b4.\nAlso, SurVerify requires at most \u23082\n(1.9\u03f5)2log\u00003\n\u03b4\u0001\u2309samples from D\u2217for validation.\nDiscussions: 1.Dimension Agnostic Tester: One of the interesting aspect of the above theorem is\nthe fact that the sample complexity of SurVerify is independent of dimension. This efficiency\nstems from the fact that the algorithm focuses on verifying the credibility of the survey data rather\nthan reconstructing the underlying regression model.\n2.Relaxing Purity of Samples: At first glance, Theorem 5 may seem limited in practical applicabil-\nity, as it assumes access to the true distribution D\u2217. However, in real-world settings, we typically\nhave access only to a distribution D\u2032that is close to D\u2217, for instance in total variation distance.\nFortunately, since the sample complexity of SurVerify isO(1)for fixed \u03f5and\u03b4, the algorithm\nremains effective in this approximate setting. By appropriately adjusting the tolerance and confi-\ndence parameters to account for the discrepancy between D\u2217andD\u2032, we can still guarantee the\ncorrectness of the testing procedure. This robustness follows directly from the Data Processing\nInequality [PW25].\n3.Dealing with Regression Models on a Subset of Dimensions: Oftentimes, broad survey data is\nused for various downstream tasks involving projections onto a small number of dimensions. How-\never, the FDD metric is not robust to arbitrary projections\u2014closeness between entire datasets does\nnot necessarily imply closeness under such projections. In these cases, the only reliable approach\nis to run SurVerify on the projected dimensions. Fortunately, the same sample from D\u2217can be\nreused across multiple projection-based checks.\n4.Fixing \u03f5, and \u03b4in practice: The choice of \u03b4is generally taken within the range of [0.01,0.1]in\npractice. Although due to the fact that the dependence of sample complexity on \u03b4is logarithmic,\nchoosing a lower value does not impact the sample complexity much. The tolerance parameter \u03b5\nshould be chosen according to the confidence required w.r.t. the underlying noise \u03b7. Given the\nfact that testing w.r.t. a lower \u03b5does not cause an increase in sample complexities in practice, one\nstrategy may be to test it with lower value of \u03b5and obtain a (constant factor) estimate to the FDD\nusing SurVerify . If there is a fixed number of samples to test with, the strategy should be to fix\nthe\u03b5level theoretically attainable according to Theorem 5.\nRequirement: Sufficient Size of the Survey Data. We show the following two-sided generaliza-\ntion bound of a general hypothesis class using the empirical Rademacher complexity.\nTheorem 6 (Generic Two-sided Generalization Bound ).Given a hypothesis set Fcontaining\nfunctions f:X \u2192 Y , and a \u00b5-lipschitz3loss function L:Y \u00d7 Y \u2192 [0, M]. LetSbe a sample set\nof size m\u22651drawn as i.i.d. samples from the distribution D, then we have with probability at least\n1\u2212\u03b4:\n\f\f\f\f\f\fE\n(x,y)\u223cD[L(f(x), y)]\u22121\nmX\n(x,y)\u2208SL(f(x), y)\f\f\f\f\f\f\u22642\u00b5b\u211cS(F) + 3M\u00a0\nlog4\n\u03b4\n2m\nNote an upper bound in the generalization bound can be found in the following textbook [MRT18].\nWe extend this to a two-sided bound controlling both under and overestimation. This is particularly\nimportant since we aim to design a tolerant tester.\n3As per the standard nomenclature, a loss function L:R\u00d7R\u2192Ris called \u00b5-Lipschitz if for any fixed\ny\u2208Randy1, y2\u2208R, we have |L(y1, y)\u2212 L(y2, y)| \u2264\u00b5|y1\u2212y2|.\n7\n\n--- Page 8 ---\nNote that computuing empirical Rademacher complexity b\u211cS(F)is known to be computationally\nhard for general hypothesis classes [FH23, MR18]. However, for a bounded weight linear and\nkernel basel class, the b\u211cS(F)admits tight analytical bounds (see [AFM20, MRT18]). We use this\nfact together with Theorem 6 to bound the sample size needed for estimating the noise variance.\nThe following result gives the size of the survey data needed for estimating fSforLasso ,Ridge\nandKernel hypothesis classes.\nLemma 7 (Minimum Survey Size for Learning Noise Variance ).Given a survey Sof size m\nwhich is sufficiently large for their respective linear hypothesis classes (see Table 1). If Assump-\ntions 1 and 2 hold, then with probability at least 1\u2212\u03b4/3we have\n\f\f\f\f\f\f\u03c32\n\u03b7\u22121\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\f\f\f\u2264\u03f5\n10,\nwhere fS\u225cargminf\u2208F1\nmP\n(x,y)\u2208S(f(x)\u2212y)2. Note that Table 1 gives the sufficient survey data\nsize from distribution for DSforLasso ,Ridge andKernel hypothesis classes.\nTable 1: Sufficient Size of Survey Data\nHypothesis Class( F)F1(Lasso )F2(Ridge )FK(Kernel )\nSize of S(M(F)) \u2126\u00c4log(d)\n\u03f52\u00e4\n\u2126\u0000d\n\u03f52\u0001\u2126\u00c4\nr2\n\u03f52\u00e4\n4\nRemark 8 (s-Sparse Linear Regression ).For the hypothesis class F1(Lasso ), one might be\ninterested in s-sparse linear regression, In that case we consider the coefficient vector \u03b8to be s-\nsparse and the hypothesis class is defined by {x\u2192 \u27e8\u03b8,x\u27e9:\u2225\u03b8\u22251\u22641,\u2225\u03b8\u22250\u2264s}. Given survey\ndata of size m= \u2126\u00c4log(s)\n\u03f52\u00e4\nfrom the distribution. If Assumptions 1 and 2 hold, then with probability\nat least 1\u2212\u03b4, we have\f\f\f\u03c32\n\u03b7\u22121\nmP\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\u2264\u03f5\n10.\nDiscussion: Relation to Out-Of-Distribution (OOD) Generalization. The OOD generalization\nliterature assumes an intrinsic model can be learned across distributions, i.e. the performance of the\nlearned hypothesis generalizes well to OOD data (Assumptions A\u2013D in [LSH+23]). Our mecha-\nnism, on the other hand, works on the case where sampling from a different distribution results in\na different model being learned. In other words, if there is an intrinsic model that can be learned\nacross distributions, the distance FDDF\nD\u2217(DS,D\u2217)for the model class Fwould be 0for all distribu-\ntionsDSandD\u2217. However, if that is not the case, we would efficiently detect whether the model\nlearned from the survey distribution DSgeneralizes well to the true distribution D\u2217.\n4.3 Lower Bound on Sample Complexity: Advantage of Not Reconstructing the Model\nSurVerify tests the model-specific credibility of a given sample survey without reconstructing\nthe model itself. The fact that we don\u2019t reconstruct the model helps us to ensure that the sample\ncomplexity is independent of the dimension. The following lemma proves that the number of sam-\nples that any algorithm that reconstructs the model to estimate model-specific distance needs grows\nlinearly with dimension.\nLemma 9 (Lower Bound on Testing with Model Reconstruction ).Under Assumption 2, and\nwhen \u03bbmin(Cov ( x))\u2265\u03bbmin, any algorithm that reconstructs the model to estimate the distance\nFDDF\nD\u2217(DS,D\u2217))within \u03f5additive error must make \u2126\u0010d\u03bbmin\u03c32\n\u03b7\n\u03f52\u0011\nqueries.\nFurthermore, if DSandD\u2217are two distributions such that their respective loss distributions are\nsubgaussian distributions with same variance but the means differ by \u03f5, then FDDD\u2217(DS,D\u2217) =\n\u03f5(by Lemma 4). Since distinguishing between the two such subgaussian distributions requires\n\u2126(1/\u03f52)we observe that the sample complexity of SurVerify is tight in terms of dependence on\n\u03f5.\n4r2is the upper bound of |K(x,x\u2032)|.\n8\n\n--- Page 9 ---\nFigure 1: Acceptance rate of SurVerify w.r.t.\nmodel class F2on Synthetic Data vs. change in \u00b5\n(over 50 runs) for \u03b4= 0.1and\u03f5= 0.05.\nFigure 2: Acceptance rate of SurVerify w.r.t.\nmodel class F1on Synthetic Data vs. change in \u00b5\n(over 50 runs) for \u03b4= 0.1and\u03f5= 0.05.\nFigure 3: Acceptance rate of SurVerify w.r.t.\nmodel class F2onACS_Income (over 50 runs) for\n\u03b4= 0.1and varying range of \u03f5.\nFigure 4: Acceptance rate of SurVerify w.r.t.\nmodel class F1onACS_Income (over 50 runs) for\n\u03b4= 0.1and varying range of \u03f5.\n5 Experimental Analysis\nIn this section, we empirically verify whether our tester SurVerify performs as per the theoretical\nanalysis. In particular, we are interested in the following research questions:\nRQ1. Does SurVerify yield accept when the survey data Sis close to being a credible dataset\nwith respect to the model class, and likewise, does SurVerify indeed reject when Sis far from be-\ning credible? Specifically, how does the acceptance rate of SurVerify change as the the distance\nbetween the survey set Sand the true distribution D\u2217, and the tolerance parameter change?\nRQ2. How many i.i.d. samples-to-test from the true distribution D\u2217doesSurVerify require to\ncertify if the survey data Sis credible? While the theoretical guarantee is for the worst-case runtime\nofSurVerify , we would like to check if SurVerify can reject a far from credible survey data\nSwith much less number of sample-to-test .\nExperimental Setup. We implement all the algorithms in Python 3.10 and use\nLinearRegression from scikit-learn to learn fS. We run our simulations on Google\nCollaboratory with 2 Intel(R) Xeon(R) CPU @ 2.20GHz, 12.7GB RAM, and 107.7GB Disk Space.\nSetup 1: Synthetic. We generate a synthetic dataset, where each coordinate of each xis generated\nfromN(0,1), and \u03b7is generated from N(0,0.1). ForDS, we generate \u03b8S\u2208R50such that each\ncoordinate is from N(0,0.01). The size of our set Sthus obtained is 100,000. For D\u2217, we generate\nthe coefficients \u03b8\u2217with each coordinate being generated from N(\u00b5,0.01)with\u00b5taking values from\n0to3at intervals of 0.1. As the value of \u00b5increases the model distance between fSandf\u2217increases.\nSetup 2: ACS_Income .As a real-world dataset, we consider the normalized ACS_Income\ndataset, which exhibits well-known fairness issues between Gender and Racial groups [DHMS21].\nWe chose Sto be generated through sampling from the subpopulation with the parameter Sex set to\n2(Female ), and the distribution D\u2217to be the subpopulation with Sex set to 1(Male ). An important\nobservation regarding this dataset is that the dataset does not satisfy the homoskedastcity assump-\ntion (Assumption 1). In particular, over 50 trials, the correlation coefficient between the response\nvariable y, and the residuals y\u2212f(x)w.r.t. the model fobtained is 0.88.\n9\n\n--- Page 10 ---\nResults and Observations. The findings from the experimental results on both the synthetic and\nthe real-world data corroborate our theoretical results. The details are as follows:\nFindings related to RQ1: We run SurVerify on each of the synthetic datasets and the\nACS_Income dataset 50 times and record the average performance and 95percentile around it.\n1.Acceptance Rate on Synthetic. In Figure 1 and 2, the BLUE curve indicates the acceptance\nrate of SurVerify on synthetic datasets described above w.r.t. F2(Ridge ) andF1(Lasso ),\nrespectively. For both the model classes of F1andF2,SurVerify exhibits similar behavior.\nItstarts with accepting all models when the difference of the coefficients, and correspondingly,\nthe model distance is small . As the difference between the coefficients, and correspondingly the\nmodel distance increase, SurVerify starts rejecting with increasing probability , and rejects all\nthe models generated with \u00b5\u22650.9,FDD\u22650.20(resp. \u00b5\u22651,FDD\u22650.21) for model class F2\n(resp.F1). The red and blue dashed vertical lines indicate the value of \u03f5and5\u03f5respectively. Hence,\nwhen the model-distance lies to the right of the blue line, SurVerify is expected to reject, whereas\nvalues to the left of the red line are expected to be accepted validating our theoretical results.\n2.Acceptance Rate on ACS_Income :In Figure 3 and 4, the BLUE line indicates the acceptance\nrate of SurVerify onACS_Income w.r.t.F2andF1, respectively. We run SurVerify with\nvarying tolerance parameter \u03f5.SurVerify always rejects for \u03f5less than 0.01, and accepts for\nhigher values. The red and blue dotted vertical lines indicate the value of FDD(DS,D\u2217)\u22480.02\nandFDD(DS,D\u2217)/5respectively. Hence, as expected, we observe that for values of \u03f5to the right\nof the red line, SurVerify is accepts more often, while for values to the left of the blue line,\nSurVerify rejects. This further indicates that the FDD(DS,D\u2217)between male and female sub-\npopulations of ACS_Income is at least 0.02with probability 0.9.\nFindings related to RQ2: Sample Complexity. In Figure 1, 2, 3 and 4, the GREEN curve demon-\nstrates #samples-to-set from DSthatSurVerify needed. As expected, in Figure 1 and 2, i.e.,\nwhile running on the synthetic dataset, as long as SurVerify accepts #samples-to-set are as per\nthe worst-case complexity. But as the distance increases and the acceptance rate of SurVerify\ndecreases, the number of #samples-to-set needed to reject also decreases. For both the model classes\nF2, andF1, the algorithm starts rejecting significantly faster once it reaches the 5\u03f5threshold.\nForACS_Income , since the FDD distance between the two distributions is 0.02,SurVerify\naccepts ( BLUE line) when \u03f5increases. In this regime, we observe the predicted 1/\u03f52decay in the\nsample complexity ( GREEN line). But when \u03f5goes smaller, SurVerify tends to reject. Specially,\nwhen \u03f5\u2264FDD(DS,D\u2217)/5 = 0 .004, the FDD distance being too far w.r.t. \u03f5, the early stopping\nkicks in and the sample complexity hits a plateau.\nIn conclusion, we observe that the effective sample complexity of test decreases as the distance of\nDSfromD\u2217increases, and the effective sample complexity of SurVerify is much lower than that\nof the worst case complexity. Extended experimental results are presented in the Appendix.\n6 Discussions, Limitations, and Future Works\nWe consider the problem of testing the credibility of survey data when used to develop a regres-\nsion model. We propose an algorithm, SurVerify , that certifies the data quality by evaluating\nand testing the FDD metric between survey and the true distribution without explicitly reconstruct-\ning the models\u2014an approach that, to the best of our knowledge, is novel in the testing literature.\nNotably, #samples-to-test required by SurVerify is independent of the data dimension, thereby\novercoming the curse of dimensionality in this context.\nIn this paper, though we provide a general framework for testing credibility, our theoretical analysis\nfocuses exclusively on linear and kernel regression models with bounded response, and homoskedas-\ntic, and non-correlated noise, which may limit its applicability. In future, it would be interesting to\nextend the model-specific credibility testing to regressions with heteroskedastic and correlated noise.\nFurthermore, it would be interesting to extending the testing framework of our algorithm beyond the\nregression models with bounded response, i.e. where closed-form Rademacher complexity based\ngeneralization bounds are not known. Furthermore, as indicated by the experiments, the proposed\nframework works for unbounded data coming from tail-bounded distributions. Thus, it will be in-\nteresting to extend the theoretical analysis to such settings.\n10\n\n--- Page 11 ---\nAcknowledgement\nThis work has been supported by the Inria-ISI, Kolkata associate team \u201cSeRAI\u201d. We also ac-\nknowledge the French National Research Agency (ANR) in the framework of the PEPR AI project\nFOUNDRY (ANR-23-PEIA-0003), and the ANR JCJC for the REPUBLIC project (ANR-22-CE23-\n0003-01) for partially supporting this work.\nReferences\n[AFM20] Pranjal Awasthi, Natalie Frank, and Mehryar Mohri. On the Rademacher complexity\nof linear hypothesis sets. CoRR, abs/2007.11045, 2020.\n[AMF20] David Alvarez-Melis and Nicol\u00f2 Fusi. Geometric dataset distances via optimal trans-\nport. In Proceedings ofthe34th International Conference onNeural Information\nProcessing Systems, NIPS \u201920, Red Hook, NY , USA, 2020. Curran Associates Inc.\n[BBC+20] Ivona Bez\u00e1kov\u00e1, Antonio Blanca, Zongchen Chen, Daniel \u0160tefankovi \u02c7c, and Eric\nVigoda. Lower bounds for testing graphical models: Colorings and antiferromagnetic\nising models. Journal ofMachine Learning Research, 21(25):1\u201362, 2020.\n[BCvV21] Antonio Blanca, Zongchen Chen, Daniel \u0160tefankovi \u02c7c, and Eric Vigoda. Hardness of\nidentity testing for restricted boltzmann machines and potts models. Journal ofMachine\nLearning Research, 22(152):1\u201356, 2021.\n[BDI+20] Abhijit Banerjee, Esther Duflo, Clement Imbert, Santhosh Mathew, and Rohini Pande.\nE-governance, accountability, and leakage in public programs: Experimental evidence\nfrom a financial management reform in india. American Economic Journal: Applied\nEconomics, 12(4):39\u201372, 2020.\n[BFR+00] Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White.\nTesting that distributions are close. In FOCS2000, pages 259\u2013269. IEEE, 2000.\n[BGKV21] Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, and N. V . Vinodchan-\ndran. Testing product distributions: A closer look. In Vitaly Feldman, Katrina\nLigett, and Sivan Sabato, editors, Proceedings ofthe32nd International Conference\nonAlgorithmic Learning Theory, volume 132 of Proceedings ofMachine Learning\nResearch, pages 367\u2013396. PMLR, 16\u201319 Mar 2021.\n[BGMV20] Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S Meel, and N. V . Vinodchandran. Effi-\ncient distance approximation for structured high-dimensional distributions via learning.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances\ninNeural Information Processing Systems, volume 33, pages 14699\u201314711. Curran\nAssociates, Inc., 2020.\n[BJ08] Silvia Balia and Andrew M Jones. Mortality, lifestyle and socio-economic status.\nJournal ofhealth economics, 27(1):1\u201326, 2008.\n[Can15] Cl\u00e9ment L. Canonne. A survey on distribution testing: Your data is big. but is it blue?\nElectron. Colloquium Comput. Complex., TR15-063, 2015.\n[Can22] Cl\u00e9ment L. Canonne. Topics and Techniques in Dis-\ntribution Testing: A Biased but Representative Sample.\nFoundations and Trends in Communications and Information Theory, 19(6):1032\u2013\n1198, 2022.\n[CDKS17] Clement L. Canonne, Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Test-\ning bayesian networks. In Satyen Kale and Ohad Shamir, editors, Proceedings ofthe\n2017 Conference onLearning Theory, volume 65 of Proceedings ofMachine Learning\nResearch, pages 370\u2013448. PMLR, 07\u201310 Jul 2017.\n11\n\n--- Page 12 ---\n[CGG+15] Hadrien Charvat, Atsushi Goto, Maki Goto, Machiko Inoue, Yoriko Heianza, Yasuji\nArase, Hirohito Sone, Tomoko Nakagami, Xin Song, Qing Qiao, et al. Impact of pop-\nulation aging on trends in diabetes prevalence: a meta-regression analysis of 160,000\njapanese adults. Journal ofdiabetes investigation, 6(5):533\u2013542, 2015.\n[CJKL22] Clement L Canonne, Ayush Jain, Gautam Kamath, and Jerry Li. The price of tolerance\nin distribution testing. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of\nThirty Fifth Conference onLearning Theory, volume 178 of Proceedings ofMachine\nLearning Research, pages 573\u2013624. PMLR, 02\u201305 Jul 2022.\n[DDK18] Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Testing ising mod-\nels. In Proceedings oftheTwenty-Ninth Annual ACM-SIAM Symposium onDiscrete\nAlgorithms, SODA \u201918, page 1989\u20132007, USA, 2018. Society for Industrial and Ap-\nplied Mathematics.\n[DGPP18] Ilias Diakonikolas, Themis Gouleakis, John Peebles, and Eric Price. Sample-optimal\nidentity testing with high probability. In Ioannis Chatzigiannakis, Christos Kakla-\nmanis, D\u00e1niel Marx, and Donald Sannella, editors, 45th International Colloquium\nonAutomata, Languages, andProgramming, ICALP 2018, July 9-13, 2018, Prague,\nCzech Republic, volume 107 of LIPIcs, pages 41:1\u201341:14. Schloss Dagstuhl - Leibniz-\nZentrum f\u00fcr Informatik, 2018.\n[DHMS21] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New\ndatasets for fair machine learning. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer,\nYann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, NuerPS, pages\n6478\u20136490, 2021.\n[DM98] Holger Dette and Axel Munk. Validation of linear regression models. TheAnnals of\nStatistics, 26(2):778\u2013800, 1998.\n[DP17] Constantinos Daskalakis and Qinxuan Pan. Square hellinger subadditivity for bayesian\nnetworks and its applications to identity testing. In Satyen Kale and Ohad Shamir,\neditors, Proceedings ofthe2017 Conference onLearning Theory, volume 65 of\nProceedings ofMachine Learning Research, pages 697\u2013703. PMLR, 07\u201310 Jul 2017.\n[DW13] John C. Duchi and Martin J. Wainwright. Distance-based and continuum fano inequal-\nities with applications to statistical estimation, 2013.\n[FH23] Vincent Froese and Christoph Hertrich. Training neural networks is np-hard in fixed\ndimension. In Advances inNeural Information Processing Systems 36: Annual\nConference onNeural Information Processing Systems 2023, NeurIPS 2023, New\nOrleans, LA,USA, December 10-16,2023, 2023.\n[GFJC+11] Robert M Groves, Floyd J Fowler Jr, Mick P Couper, James M Lepkowski, Eleanor\nSinger, and Roger Tourangeau. Survey methodology. John Wiley & Sons, 2011.\n[GoC24] Statistics Canada Government of Canada. Learning resources: Statistics:\nPower from data! non-probability sampling. https://www150.statcan.gc.ca/n1/edu\n/power-pouvoir/ch13/nonprob/5214898-eng.htm, 2024.\n[GS02] Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics.\nInternational statistical review, 70(3):419\u2013435, 2002.\n[HWB17] Steven G Heeringa, Brady T West, and Patricia A Berglund. Applied survey data\nanalysis. chapman and hall/CRC, 2017.\n[IK20] Michael Isakov and Shiro Kuriwaki. Towards Principled Unskewing: Viewing 2020\nElection Polls Through a Corrective Lens From 2016. Harvard Data Science Review,\n2(4), nov 3 2020. https://hdsr.mitpress.mit.edu/pub/cnxbwum6.\n[JWHT21] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. AnIntroduction\ntoStatistical Learning: with Applications inR. Springer US, 2021.\n12\n\n--- Page 13 ---\n[Kal21] Graham Kalton. Introduction toSurvey Sampling. SAGE Publications, Inc., Thousand\nOaks, California, May 2021.\n[KKM+21] Christopher T Kenny, Shiro Kuriwaki, Cory McCartan, Evan TR Rosenman, Tyler\nSimko, and Kosuke Imai. The use of differential privacy for census data and its impact\non redistricting: The case of the 2020 us census. Science advances, 7(41):eabk3283,\n2021.\n[Lei20] Jing Lei. Convergence and concentration of empirical measures under Wasserstein\ndistance in unbounded functional spaces. Bernoulli, 26(1):767 \u2013 798, 2020.\n[Loh21] Sharon L Lohr. Sampling: design andanalysis. Chapman and Hall/CRC, 2021.\n[LSH+23] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng\nCui. Towards out-of-distribution generalization: A survey, 2023.\n[LT91] Michel Ledoux and Michel Talagrand. Probability inBanach Spaces. Springer Berlin\nHeidelberg, 1991.\n[Mau17] Andrew Maul. Rethinking traditional methods of survey validation. Measurement:\nInterdisciplinary Research andPerspectives, 15(2):51\u201369, 2017.\n[MR18] Pasin Manurangsi and Daniel Reichman. The computational complexity of training\nrelu(s). CoRR, abs/1810.04207, 2018.\n[MRT18] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations ofmachine\nlearning. MIT Press, 2018.\n[MS17] Esther L Meerwijk and Jae M Sevelius. Transgender population size in the united\nstates: a meta-regression of population-based probability samples. American journal\nofpublic health, 107(2):e1\u2013e8, 2017.\n[Pan08] Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled\ndiscrete data. IEEE Transactions onInformation Theory, 54(10):4750\u20134755, 2008.\n[Pan17] Wen-Tsao Pan. A newer equal part linear regression model: A case study of the influ-\nence of educational input on gross national income. Eurasia Journal ofMathematics,\nScience andTechnology Education, 13(8):5765\u20135773, 2017.\n[PC84] Richard R. Picard and R. Dennis Cook. Cross-validation of regression models. Journal\noftheAmerican Statistical Association, 79(387):575\u2013583, 1984.\n[PW25] Yury Polyanskiy and Yihong Wu. Information Theory: From Coding toLearning.\nCambridge University Press, 2025.\n[RL03] Peter J Rousseeuw and Annick M Leroy. Robust regression andoutlier detection. John\nwiley & sons, 2003.\n[SD94] Priscilla Salant and Don A Dillman. How to conduct your own survey. Willey, 1994.\n[Sne77] Ronald D. Snee. Validation of regression models: Methods and examples.\nTechnometrics, 19(4):415\u2013428, 1977.\n[SS12] Elias M. Stein and Rami Shakarchi. Functional Analysis. Princeton University Press,\nPrinceton, 2012.\n[Stu97] Winfried Stute. Nonparametric model checks for regression. TheAnnals ofStatistics,\n25(2):613 \u2013 641, 1997.\n[SZ21] Steven J Staffa and David Zurakowski. Statistical development and validation of clini-\ncal prediction models. Anesthesiology, 135(3):396\u2013405, September 2021.\n[VV17] Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal\nidentity testing. SIAM Journal onComputing, 46(1):429\u2013455, 2017.\n13\n\n--- Page 14 ---\n[Was04] Larry Wasserman. AllofStatistics: AConcise Course inStatistical Inference. Springer\nTexts in Statistics. Springer, New York, 2004.\n[ZL20] Puning Zhao and Lifeng Lai. Minimax optimal estimation of KL divergence for con-\ntinuous distributions. IEEE Trans. Inf.Theory, 66(12):7787\u20137811, 2020.\n14\n\n--- Page 15 ---\nAppendix\nTable of Contents\nAFDD-variance Decomposition of Loss: Proof of Lemma 4 16\nB Generic Two-sided Generalization Bounds: Proof of Thoerem 6 16\nC Minimum Survey Size for Learning Noise Variance: Proof of Lemma 7 19\nC.1 From Two-sided Generalization Bound to Estimating Noise Variance . . . . . . 19\nC.2 From Generalization Bound to Noise variance for Linear and Kernel Classes . . 20\nC.3 From Noise Variance Bounds to Minimum Survey Size: Proof of Lemma 7 . . . 22\nD Correctness of SurVerify and Sample Complexity: Proof of Theorem 5 23\nE Lower Bound for Model Reconstruction 25\nF Experimental Results 26\n15\n\n--- Page 16 ---\nAFDD-variance Decomposition of Loss: Proof of Lemma 4\nLemma 4 (FDD-variance Decomposition of Loss) .If the model class Fsatisfies Assumption 1, then\nE\n(x,y)\u223cD\u2217\u00ee\n(fS(x)\u2212y)2\u00f3\n= (FDDF\nD\u2217(DS,D\u2217))2+\u03c32\n\u03b7.\nProof. Observe that\nE\n(x,y)\u223cD\u2217\u00ee\n(fS(x)\u2212y)2\u00f3\n=E\nD\u2217,\u03b7\u00ee\n(fS(x)\u2212f\u2217(x)\u2212\u03b7)2\u00f3\n=E\nD\u2217,\u03b7\u00ee\n(fS(x)\u2212f\u2217(x))2+\u03b72\u22122 (fS(x)\u2212f\u2217(x))\u03b7\u00f3\n=E\nD\u2217\u00ee\n(fS(x)\u2212f\u2217(x))2\u00f3\n+E\n\u03b7\u0002\u03b72\u0003\u22122E\nD\u2217E\n\u03b7[(fS(x)\u2212f\u2217(x))\u03b7]\n=E\nD\u2217\u00ee\n(fS(x)\u2212f\u2217(x))2\u00f3\n+\u03c32\n\u03b7+ 2E\nD\u2217\u00ef\n(fS(x)\u2212f\u2217(x))E\n\u03b7[\u03b7]\u00f2\nFrom Assumption 1\n=E\nD\u2217\u00ee\n(fS(x)\u2212f\u2217(x))2\u00f3\n+\u03c32\n\u03b7\n=dist2\nD\u2217(fS, f\u2217) +\u03c32\n\u03b7\nB Generic Two-sided Generalization Bounds: Proof of Thoerem 6\nBefore proving the theorem, we state the following results that are relevant to our proof:\nLemma 10 (Talagrand\u2019s Contraction Lemma [LT91]) .Given a real-valued \u00b5-lipschitz loss func-\ntionL, a sample set Sand a hypothesis class Fof real valued function, the following inequality\nholds:\nb\u211cS(L \u25e6 F )\u2264\u00b5b\u211cS(F)\nLemma 11 (McDiarmid\u2019s Inequality [MRT18]) .LetX1, X2, . . . , X m\u2208 Xmbe iid random vari-\nables and there exists a constant csuch that f:Xm\u2192Rsatisfies:\n|f(x1, . . . , x i, . . . , x m)\u2212f(x1, . . . , x\u2032\ni, . . . , x m)| \u2264c,\u2200x1, . . . , x m, x\u2032\ni\u2208Rd\nThen, for any \u03f5 >0the following inequality hold:\nPr [|f(X1, . . . , X m)\u2212E[f(X1, . . . , X m)]| \u2265\u03f5]\u22642 exp\u00c5\u22122\u03f52\nmc2\u00e3\nWe also introduce the definition of Rademacher Complexity that only depends on the class of func-\ntions under consideration\nDefinition 12 (Rademacher Complexity [MRT18]) .LetSbe a sample set of size m\u22651drawn as\ni.i.d. samples from the distribution D. Then, the Rademacher Complexity of Gis the expectation of\nthe empirical Rademacher complexity over all samples of size mdrawn from D:\nb\u211cm(G) = E\nS\u223cDm\u00eeb\u211cS(G)\u00f3\nThe next result is the intermediate lemma required, which quantifies how well the empirical mean\nestimates the true expectation over a bounded function class, in terms of its empirical Rademacher\ncomplexity (Definition 2).\nLemma 13 (Two-sided Rademacher Bound for Bounded Functions ).Given a family of functions\nGcontaining functions g:Z \u2192 [0, M]. LetSbe a sample set of size m\u22651drawn as i.i.d. samples\nfrom the distribution D. Then with probability at least 1\u2212\u03b4for all g\u2208 G:\f\f\f\f\f\f1\nmX\ni\u2208[m]g(z)\u2212E[g(z)]\f\f\f\f\f\f\u22642b\u211cS(G) + 3M\u00a0\nlog4\n\u03b4\n2m\n16\n\n--- Page 17 ---\nProof. For a given sample set Sof size m, let us denote by \u02c6ES[g]the empirical loss1\nmP\ni\u2208[m]g(zi).\nConsequently, we define a function \u03a6corresponding of a sample set Sas:\n\u03a6(S) = sup\ng\u2208G\u02c6ES[g]\u2212E[g]\nWe first upper bound the expectation of this function \u03a6(S)overS\u2208 Dm.\nE\nS[\u03a6(S)] =E\nS\u00f1\nsup\ng\u2208G\u02c6ES[g]\u2212E[g]\u00f4\n=E\nS\u00f1\nsup\ng\u2208GE\nS\u2032\u00ee\u02c6ES[g]\u2212\u02c6ES\u2032[g]\u00f3\u00f4\n\u2264E\nS,S\u2032\u00f1\nsup\ng\u2208G\u02c6ES[g]\u2212\u02c6ES\u2032[g]\u00f4\nBy Jensen\u2019s Inequality\n=E\nS,S\u2032\uf8ee\n\uf8f0sup\ng\u2208G1\nmX\ni\u2208[m](g(zi)\u2212g(z\u2032\ni))\uf8f9\n\uf8fb\n=E\nr,S,S\u2032\uf8ee\n\uf8f0sup\ng\u2208G1\nmX\ni\u2208[m]ri(g(zi)\u2212g(z\u2032\ni))\uf8f9\n\uf8fb Expectation over S, S\u2032\n\u2264E\nr,S,S\u2032\uf8ee\n\uf8f0sup\ng\u2208G1\nmX\ni\u2208[m]rig(zi) + sup\ng\u2208G1\nmX\ni\u2208[m]\u2212rig(z\u2032\ni)\uf8f9\n\uf8fb sup(a+b)\u2264sup(a) + sup( b)\n=E\nr,S\u2032\uf8ee\n\uf8f0sup\ng\u2208G1\nmX\ni\u2208[m]rig(zi)\uf8f9\n\uf8fb+E\nr,S\uf8ee\n\uf8f0sup\ng\u2208G1\nmX\ni\u2208[m]\u2212rig(z\u2032\ni)\uf8f9\n\uf8fb Linearity of expectations\n=2E\nr,S\uf8ee\n\uf8f0sup\ng\u2208G1\nmX\ni\u2208[m]rig(zi)\uf8f9\n\uf8fb= 2\u211c(G) (1)\nNow, we will use the McDiarmid\u2019s inequality(Lemma 11) on this function. For that purpose, observe\nthat each coordinate of the input essentially corresponds to one of the data points in the sample.\nWe use this fact and the boundedness of gto obtain our prerequisite bound to apply McDiarmid\u2019s\ninequality. Let us consider two sample sets SandS\u2032that differs at exactly one sample point, say the\ni-th location. Then, we have:\n\u03a6 (S)\u2212\u03a6 (S\u2032)\u2264sup\ng\u2208G\u00c4\u02c6ES[g]\u2212\u02c6ES\u2032[g]\u00e4\n= sup\ng\u2208Gg(zi)\u2212g(z\u2032\ni)\nm\u2264M\nm\nHere, the first inequality follows from the fact that supx(f(x)\u2212g(x))\u2265supxf(x)\u2212supxg(x),\nand Now, by McDiarmid\u2019s inequality, we have,\nPr\uf8ee\n\uf8f0\u03a6(S)\u2212E[\u03a6(S)]\u2265M\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u2264exp\u00c7\n\u22122M2m2log4\n\u03b4\n2m2M2\u00e5\n=\u03b4\n4(2)\nCombining equations (1) and (2), we have for all g\u2208 G:\nPr\uf8ee\n\uf8f01\nmX\ni\u2208[m]g(z)\u2212E[g(z)]\u22652\u211c(G) +M\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u2264\u03b4\n4(3)\nNow, we bound the empirical Rademacher sample complexity in terms of the Rademacher com-\nplexity. We again consider two sample sets S, and S\u2032that differs at exactly one point, say zi. Then,\nusing the fact that supx(f(x)\u2212g(x))\u2265supxf(x)\u2212supxg(x), we get\nb\u211cS(G)\u2212b\u211cS\u2032(G)\u22641\nmE\u00f1\nsup\ng\u2208G(ri(g(zi)\u2212g(z\u2032\ni)))\u00f4\n\u2264M\nm\n17\n\n--- Page 18 ---\nNow, by McDiarmid\u2019s Ineqality(Lemma 11), we have:\nPr\uf8ee\n\uf8f0\u211c(G)\u2212b\u211cS(G)\u2265M\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u2264\u03b4\n4(4)\nNow, we combine equations (3) and (4) through an union bound to obtain:\nPr\uf8ee\n\uf8f01\nmX\ni\u2208[m]g(z)\u2212E[g(z)]\u22652b\u211cS(G) + 3M\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u2264\u03b4\n2\nSimilarly, we can show:\nPr\uf8ee\n\uf8f0E[g(z)]\u22121\nmX\ni\u2208[m]g(z)\u22652b\u211cS(G) + 3M\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u2264\u03b4\n2\nCombining through a union bound, we get the desired result.\nProof of Theorem 6. Now, we are ready to give the proof of Theorem 6. A restatement of the\ntheorem is given below.\nTheorem 6 (Generic Two-sided Generalization Bound ).Given a hypothesis set Fcontaining\nfunctions f:X \u2192 Y , and a \u00b5-lipschitz5loss function L:Y \u00d7 Y \u2192 [0, M]. LetSbe a sample set\nof size m\u22651drawn as i.i.d. samples from the distribution D, then we have with probability at least\n1\u2212\u03b4:\n\f\f\f\f\f\fE\n(x,y)\u223cD[L(f(x), y)]\u22121\nmX\n(x,y)\u2208SL(f(x), y)\f\f\f\f\f\f\u22642\u00b5b\u211cS(F) + 3M\u00a0\nlog4\n\u03b4\n2m\nProof. From Lemma 13, we know the two-sided deviation on the empirical mean w.r.t true expection\nover a bounded function class Gcontaining functions g:Z \u2192 [0, M],\nPr\uf8ee\n\uf8f0\f\f\f\f\f\f1\nmX\ni\u2208[m]g(z)\u2212E[g(z)]\f\f\f\f\f\f\u22642b\u211cS(G) + 3M\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4 (5)\nTakeGto be the set of loss functions L:Y \u00d7 Y \u2192 [0, M], then for any f\u2208 F we can write\ninequality (6) as,\nPr\uf8ee\n\uf8f0\f\f\f\f\f\fE\n(x,y)\u223cD[L(f(x), y)]\u22121\nmX\n(x,y)\u2208SL(f(x), y)\f\f\f\f\f\f\u22642b\u211cS(L \u25e6 F ) + 3M\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4(6)\nFrom Talagrand\u2019s Contraction Lemma 10, we have\nb\u211cS(L \u25e6 F )\u2264\u00b5b\u211cS(F) (7)\nPlugging back inequality (7) in (6) we get the following with probability at least 1\u2212\u03b4:\nPr\uf8ee\n\uf8f0\f\f\f\f\f\fE\n(x,y)\u223cD[L(f(x), y)]\u22121\nmX\n(x,y)\u2208SL(f(x), y)\f\f\f\f\f\f\u22642\u00b5b\u211cS(F) + 3M\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4\nthis completes the proof.\n5As per the standard nomenclature, a loss function L:R\u00d7R\u2192Ris called \u00b5-Lipschitz if for any fixed\ny\u2208Randy1, y2\u2208R, we have |L(y1, y)\u2212 L(y2, y)| \u2264\u00b5|y1\u2212y2|.\n18\n\n--- Page 19 ---\nC Minimum Survey Size for Learning Noise Variance: Proof of Lemma 7\nThis section is organized into three parts. In subsection C.1, we establish a two-sided generalization\nbound for the estimation of noise variance in the regression model, in terms of empirical rademacher\ncomplexity. In subsection C.2 we extends this result to both bounded linear and kernel hypothesis\nclasses using their corresponding rademacher bounds. Finally, In subsection C.3 presents the proof\nof Lemma 7, which formalizes the minimum survey size required for estimating noise variance.\nC.1 From Two-sided Generalization Bound to Estimating Noise Variance\nThe next result uses Theorem 6 applied to the squared loss setting, and combines it with the assump-\ntions specific to linear regression over bounded domains to estimate the noise variance.\nLemma 14 (Concentration of Empirical Squared Loss around Noise Variance ).Given a lin-\near regression model f\u2217:y=f\u2217(x) +\u03b7, where \u03b7is the zero-mean additive noise term with\nvariance Var(x,y)\u223cD\u2217(\u03b7) = \u03c32\n\u03b7. Let Sbe a sample set of size m\u22651drawn as i.i.d. sam-\nples from the distribution DS. If Assumptions 1 and 2 holds, then the regression model fS\u2190\nargminf\u2208F1\nmP\n(x,y)\u2208S(f(x)\u2212y)2satisfies, with probability at least 1\u2212\u03b4:\n\f\f\f\f\f\f\u03c32\n\u03b7\u22121\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\f\f\f\u22648\u201cRS(F) + 12\u00a0\nlog4\n\u03b4\n2m\nProof. We split the proof into two steps,\nStep 1: Generalization bound for squared loss. In this step, we show the two-sided generalization\nbound for squared loss. From Theorem 6, We consider the squared loss L(f(x), y) = (f(x)\u2212y)2\nfor all (x, y)\u2208 X \u00d7 Y andf\u2208 F.\nFrom assumption 2, we bound the maximum value of the loss function:\nM= max\ny,y\u2032L(y, y\u2032) = max\ny,y\u2032(y\u2212y\u2032)2\u22644 (8)\nSimilarly, for the Lipschitzness of the loss function, we have:\n\u00b5\u2264max\ny,y\u2032\u2207yL(y, y\u2032) = max\ny,y\u2032\u2207y(y\u2212y\u2032)2= max\ny,y\u20322 (y\u2212y\u2032)\u22644 (9)\nNow, applying Theorem 6 to the squared loss and function class F, we get,\nPr\uf8ee\n\uf8f0\f\f\f\f\f\fE\n(x,y)\u223cD\u0002(f(x)\u2212y)2\u0003\u22121\nmX\n(x,y)\u2208S(f(x)\u2212y)2\f\f\f\f\f\f\u22648\u201cRS(F) + 12\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4(10)\nStep 2: Concentration of noise variance. We now show that the empirical squared loss of the\nestimator fSconcentrates around the true noise variance \u03c32\n\u03b7, using the generalization bound from\nStep 1 and Assumption 1. We prove the upper and lower bounds separately.\nUpper Bound: Letf\u2217is the optimal linear regression model on the true distribution D\u2217. Let f\u2217\nSis\nthe optimal linear regression model on the survey distribution DS.\nFrom Assumption 1, we have:\n\u03c32\n\u03b7=E\n(x,y)\u223cD\u2217[(f\u2217(x)\u2212y)2] = E\n(x,y)\u223cDS[(f\u2217\nS(x)\u2212y)2] (11)\nTherefore,\n\u03c32\n\u03b7=E\n(x,y)\u223cDS[(f\u2217\nS(x)\u2212y)2]\u2264E\n(x,y)\u223cDS[(fS(x)\u2212y)2] (12)\nThe inequality (12) comes from the optimality of f\u2217\nSonDS.\n19\n\n--- Page 20 ---\nNow, applying the upper-sided generalization bound in (10) with f=fS, we have\nPr\uf8ee\n\uf8f0E\n(x,y)\u223cDS[(fS(x)\u2212y)2]\u22641\nmX\n(x,y)\u2208S(fS(x)\u2212y)2+ 8\u201cRS(F) + 12\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4\n2(13)\nCombining (12) and (13) we get,\nPr\uf8ee\n\uf8f0\u03c32\n\u03b7\u22641\nmX\n(x,y)\u2208S(fS(x)\u2212y)2+ 8\u201cRS(F) + 12\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4\n2(14)\nLower Bound: We now show the lower bound, by applying lower-sided generalization bound\nin (10) with f=f\u2217\nSwe get,\nPr\uf8ee\n\uf8f0E\n(x,y)\u223cDS\u0002(f\u2217\nS(x)\u2212y)2\u0003\u22651\nmX\n(x,y)\u2208S(f\u2217\nS(x)\u2212y)2\u22128\u201cRS(F)\u221212\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4\n2\n(15)\nSince fSis the optimal regression model over the empirical loss. Therefore,\nX\n(x,y)\u2208S(f\u2217\nS(x)\u2212y)2\u2265X\n(x,y)\u2208S(fS(x)\u2212y)2(16)\nUsing inequality (16) in (15), we get\nPr\uf8ee\n\uf8f0E\n(x,y)\u223cD\u0002(f\u2217\nS(x)\u2212y)2\u0003\u22651\nmX\ni\u2208[m](fS(x)\u2212y)2\u22128\u201cRS(F)\u221212\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4\n2(17)\nNow, Combining (11) and (17) we get,\nPr\uf8ee\n\uf8f0\u03c32\n\u03b7\u22651\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\u22128\u201cRS(F)\u221212\u00a0\nlog4\n\u03b4\n2m\uf8f9\n\uf8fb\u22651\u2212\u03b4\n2(18)\nCombining the upper bound (14) and lower bound (18) using the union bound we get, with proba-\nbility at least 1\u2212\u03b4:\n\f\f\f\f\f\f\u03c32\n\u03b7\u22121\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\f\f\f\u22648\u201cRS(F) + 12\u00a0\nlog4\n\u03b4\n2m.\nThis completes the proof.\nC.2 From Generalization Bound to Noise variance for Linear and Kernel Classes\nIn this section, We show the general two-sided generalization bound for the empirical squared loss\nfrom Lemma 14 for specific families of hypothesis classes. In particular, we consider:\n\u2022 Linear function classes with bounded \u21131and\u21132norms, corresponding to Lasso and\nRidge regression respectively.\n\u2022 Kernel-based functions classes with bounded RKHS norm, corresponding to Kernel .\nIn each case, we use upper bounds on the Rademacher complexity for the corresponding class, and\nthen apply Lemma 14 to obtain corresponding generalization guarantees.\nCase: F1(Lasso )andF2(Ridge )\n[AFM20] has proved the following upper bound of the empirical Rademacher complexity for\nbounded linear hypothesis classes.\n20\n\n--- Page 21 ---\nLemma 15 (Empirical Rademacher Complexity of Bounded Linear Hypothesis [AFM20]) .\nLetFp={x\u2192 \u27e8\u03b8,x\u27e9:\u2225\u03b8\u2225p\u22641}be a family of linear functions defined over Rdwith bounded\nweight in \u2113p-norm where p\u2208 {1,2}. Let S= (x1,x2, . . . ,xm)be a sample of size m. Then, the\nempirical Rademacher complexity of Fpis upper bounded by:\n\u201cRS(Fp)\u2264\u00ae1\nmp\n2 log(2 d)\n\nXT\n\n2,\u221eifp= 1\n1\nm\n\nXT\n\n2,2ifp= 2\nwhere Xis ad\u00d7mmatrix with xi\u2019s as columns: X= [x1. . .xm]\nLemma 16 (Two-sided Generalization Bound of \u21131and\u21132bounded linear hypothesis class ).\nLetFp={x\u2192 \u27e8\u03b8,x\u27e9:\u2225\u03b8\u2225p\u22641}. Given a linear regression model f\u2217:y=f\u2217(x) +\u03b7, where \u03b7\nis the zero-mean additive noise term with variance Var(x,y)\u223cD\u2217(\u03b7) =\u03c32\n\u03b7. Given a sample Sof size\nmsampled i.i.d from a distribution DS. If Assumption 1 and 2 holds, then the regression model\nfS\u2190argmin\nf\u2208Fp1\nmX\n(x,y)\u2208S(f(x)\u2212y)2\nsatisfies, with probability at least 1\u2212\u03b4:\n\f\f\f\f\f\f\u03c32\n\u03b7\u22121\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\f\f\f\u2264\uf8f1\n\uf8f2\n\uf8f38\u00bb\n2 log(2 d)\nm+ 12q\nlog4\n\u03b4\n2mIfp= 1 (Lasso )\n8\u00bb\nd\nm+ 12q\nlog4\n\u03b4\n2mIfp= 2 (Ridge )\nProof. From Assumption 2, we have \u2225x\u2225\u221e\u22641for all x\u2208 X . Therefore, each column xiof the\nmatrix X\u2208Rd\u00d7msatisfies \u2225xi\u2225\u221e\u22641. then\n\nXT\n\n2,\u221e\u2264\u221amand\n\nXT\n\n2,2\u2264\u221a\ndm.\nForp= 1(Lasso ):From Lemma 15, we get:\n\u201cRS(F1)\u22641\nm\u00bb\n2 log(2 d)\n\nXT\n\n2,\u221e\n\u22641\nm\u00bb\n2 log(2 d)\u00b7\u221am=\u00a0\n2 log(2 d)\nm\nForp= 2(Ridge ):Again, from Lemma 15,\n\u201cRS(F2)\u22641\nm\n\nXT\n\n2,2\u22641\nm\u221a\ndm=\u2026\nd\nm\nNow, plugging the above bounds on \u201cRS(Fp)into Lemma 14 yields:\n\f\f\f\f\f\f\u03c32\n\u03b7\u22121\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\f\f\f\u22648\u201cRS(Fp) + 12\u00a0\nlog4\n\u03b4\n2m\nwhich gives the desired bounds:\nLasso :8\u00a0\n2 log(2 d)\nm+ 12\u00a0\nlog4\n\u03b4\n2m\nRidge :8\u2026\nd\nm+ 12\u00a0\nlog4\n\u03b4\n2m\nCase: FK(Kernel )\nWe define the hypothesis class as:\nFKdef={x7\u2192 \u27e8\u03b8,\u03a6(x)\u27e9H:\u2225\u03b8\u2225H\u22641}\n21\n\n--- Page 22 ---\nwhere \u03a6:X \u2192 His the feature map associated with a positive definite symmetric (PDS) kernel\nK:X \u00d7 X \u2192 R.\nWe first recall the following Rademacher complexity bound for kernel regression from [MRT18,\nTheorem 6.12]:\nLemma 17 (PDS Kernel Rademacher Complexity Bound [MRT18]) .LetKbe a PDS kernel\nwith associated feature map \u03a6satisfying K(x,x) =\u2225\u03a6(x)\u22252\nH\u2264r2for all x\u2208 X. Then, for any\ni.i.d. sample Sof size m, the empirical Rademacher complexity of FKsatisfies:\nb\u211cS(FK)\u2264r\u221am\nLemma 18 (Two-sided Generalization Error of Bounded Kernel Hypothesis ).LetFKbe defined\nas above and suppose K(x,x)\u2264r2for all x\u2208 X . Given a linear regression model f\u2217:y=\nf\u2217(x) +\u03b7, where \u03b7is the zero-mean additive noise term with variance Var(x,y)\u223cD\u2217(\u03b7) =\u03c32\n\u03b7and\nsample Sof size msampled i.i.d from a distribution DS. If Assumptions 1 and 2 holds, then the\nregression model\nfS\u2190argmin\nf\u2208FK1\nmX\n(x,y)\u2208S(f(x)\u2212y)2\nsatisfies, with probability at least 1\u2212\u03b4:\n\f\f\f\f\f\f\u03c32\n\u03b7\u22121\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\f\f\f\u22648r\u221am+ 12\u00a0\nlog4\n\u03b4\n2m.\nProof. From Lemma 17, we have:\nb\u211cS(FK)\u2264r\u221am.\nPlugging this into the general bound from Lemma 14 obtains the stated result.\nC.3 From Noise Variance Bounds to Minimum Survey Size: Proof of Lemma 7\nWe now translate the generalization error bounds derived for Lasso ,Ridge andKernel into\nsample size guarantees for estimating the noise variance, which leads to the proof of Lemma 7.\nLemma 7 (Minimum Survey Size for Learning Noise Variance ).Given a survey Sof size m\nwhich is sufficiently large for their respective linear hypothesis classes (see Table 1). If Assump-\ntions 1 and 2 hold, then with probability at least 1\u2212\u03b4/3we have\n\f\f\f\f\f\f\u03c32\n\u03b7\u22121\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\f\f\f\u2264\u03f5\n10,\nwhere fS\u225cargminf\u2208F1\nmP\n(x,y)\u2208S(f(x)\u2212y)2. Note that Table 1 gives the sufficient survey data\nsize from distribution for DSforLasso ,Ridge andKernel hypothesis classes.\nTable 2: Sufficient Size of Survey Data\nHypothesis Class( F)F1(Lasso )F2(Ridge )FK(Kernel )\nSize of S(M(F)) \u2126\u00c4log(d)\n\u03f52\u00e4\n\u2126\u0000d\n\u03f52\u0001\u2126\u00c4\nr2\n\u03f52\u00e4\nProof. From Lemmas 16 and 18, we have that with probability at least 1\u2212\u03b4,\n\f\f\f\f\f\f\u03c32\n\u03b7\u22121\nmX\n(x,y)\u2208S(fS(x)\u2212y)2\f\f\f\f\f\f\u2264\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f38\u00bb\n2 log(2 d)\nm+ 12q\nlog4\n\u03b4\n2mifp= 1 (Lasso )\n8\u00bb\nd\nm+ 12q\nlog4\n\u03b4\n2mifp= 2 (Ridge )\n8r\u221am+ 12q\nlog4\n\u03b4\n2mKernel(19)\n22\n\n--- Page 23 ---\nWe now choose mlarge enough so that each term on the right-hand side of (19) is at most \u03f5/20,\nensuring the total bound is at most \u03f5/10.\nForp= 1(Lasso ):Set\nm\u2265max\u00df\n51200log(2 d)\n\u03f52,28800log(4 /\u03b4)\n\u03f52\u2122\nThen,\nm\u226551200log(2 d)\n\u03f52=\u21d28\u00a0\n2 log(2 d)\nm\u2264\u03f5\n20\nm\u226528800log(4 /\u03b4)\n\u03f52=\u21d212\u00a0\nlog4\n\u03b4\n2m\u2264\u03f5\n20\nSumming the two terms in (19) gives a bound of at most \u03f5/10.\nForp= 2 (Ridge ):Similarly, taking\nm\u2265max\u00df\n51200d\n\u03f52,28800log(4 /\u03b4)\n\u03f52\u2122\nyields the desired bound.\nForKernel :Taking\nm\u2265max\u00df\n25600r2\n\u03f52,28800log(4 /\u03b4)\n\u03f52\u2122\nensures that each term on the right-hand side of the kernel bound in (19) is at most \u03f5/20, completing\nthe proof.\nD Correctness of SurVerify and Sample Complexity: Proof of Theorem 5\nNow, we present the proof of the correctness of our algorithm. The restatement of the theorem is\ngiven below.\nTheorem 5 (Correctness of SurVerify and Sample Complexity ).Given a survey sample S\n(drawn from an unknown distribution DS), a model class Fand i.i.d. sampling access to the true\ndistribution D\u2217then for any \u03f5and\u03b4\u2208(0,1), if the size of Sis large enough (Table 1) then\n1. If(FDDF\nD\u2217(DS,D\u2217))2\u2264\u03f5, then SurVerify outputs ACCEPT with probability 1\u2212\u03b4.\n2. If(FDDF\nD\u2217(DS,D\u2217))2>5\u03f5, then SurVerify outputs REJECT with probability 1\u2212\u03b4.\nAlso, SurVerify requires at most \u23082\n(1.9\u03f5)2log\u00003\n\u03b4\u0001\u2309samples from D\u2217for validation.\nProof. The sample complexity of the algorithm can be easily seen from the algorithm. The main\nthing to prove is the correctness of the algorithm. We will prove the two parts separately. To start,\nwe observe that from Lemma 7, we have\nPrh\f\f\f\u03c32\n\u03b7\u2212\u02c6LS\f\f\f>0.1\u03f5i\n\u2264\u03b4\n3(20)\nLet\u02c6\u03b3tbe the value of \u02c6\u03b3aretrounds of the while loop. From the Linearity of Expectation, we have\nE[\u02c6\u03b3t] =X\n(x,y)\u2208SD\u2217E\n(x,y)\u223cD\u2217\u0002(fS(x)\u2212y)2\u0003=tE\n(x,y)\u223cD\u2217\u0002(fS(x)\u2212y)2\u0003(21)\nFrom Assumption 2, we have (fS(xi)\u2212y)2\u2208[0,4]for all i\u2208[t]. Since each of the tindepen-\ndent variables is bounded, we now use Hoeffding\u2019s inequality to bound the deviation of \u02c6\u03b3tfrom its\nexpectation.\nProof of 5 In this case we have to bound the probability that SurVerify outputs REJECT at any\nof the t-iterations of the while loop or in the ifstatement at the end (line 11 to 14).\n23\n\n--- Page 24 ---\nby Hoeffding\u2019s Lemma, at any round t\u2208[\u03c4], we have\nPr\u00f1\n\u02c6\u03b3\u2212tE\n(x,y)\u223cD\u2217\u00ee\n(fS(x)\u2212y)2\u00f3\n>\u221a\nt\u00a0\n2 log\u00c53\u03c4\n\u03b4\u00e3\u00f4\n\u2264exp\u00c5\n\u2212log\u00c53\u03c4\n\u03b4\u00e3\u00e3\n=\u03b4\n3\u03c4(22)\nIfdist2\nD\u2217(fS, f\u2217)\u2264\u03f5, then from Lemma 4\nE\n(x,y)\u223cD\u2217\u00ee\n(fS(x)\u2212y)2\u00f3\n=dist D\u2217(fS, f\u2217) +\u03c32\n\u03b7\u2264\u03f5+\u03c32\n\u03b7 (23)\nThus if dist2\nD\u2217(fS, f\u2217)\u2264\u03f5and|\u03c32\n\u03b7\u2212\u02c6LS| \u22640.1\u03f5then\nE\n(x,y)\u223cD\u2217\u00ee\n(fS(x)\u2212y)2\u00f3\n\u22641.1\u03f5+\u02c6LS (24)\nCombining (22) and (24) using union bound, we get at any round t,\nPr\u00f1\n\u02c6\u03b3\u2212t\u02c6LS>1.1t\u03f5+\u00a0\n2tlog\u00c53\u03c4\n\u03b4\u00e3\u00f4\n\u2264\u03b4\n3\u03c4(25)\nSo, if dist2\nD\u2217(fS, f\u2217)\u2264\u03f5and|\u03c32\n\u03b7\u2212\u02c6LS| \u22640.1\u03f5then the probability that SurVerify output\nREJECT in the while loop is at most \u03b4/3. Also, at the end of the while loop let \u02c6\u03b3\u03c4be the value of\n\u02c6\u03b3. The value of \u03c4has been so chosen that\nPr\u00ef\f\f\f\f\u02c6\u03b3\u03c4\u2212\u03c4E\nD\u2217\u0002(fS(x)\u2212y)2\u0003\f\f\f\f>1.9\u03c4\u03f5\u00f2\n\u22642 exp\u00c5\n\u22122\u03c42(1.9\u03f5)2\n4\u03c4\u00e3\n=\u03b4\n3. (26)\nCombining Equation (26) and (24) we see that if dist2\nD\u2217(fS, f\u2217)\u2264\u03f5and|\u03c32\n\u03b7\u2212\u02c6LS| \u22640.1\u03f5then\nPr\u00ee\n\u02c6\u03b3\u03c4\u2212\u03c4\u02c6LS>3\u03c4\u03f5\u00f3\n\u2264\u03b4\n3. (27)\nFinally, combining Equation (27), (25) and (20) we have that if dist2\nD\u2217(fS, f\u2217)\u2264\u03f5then probabil-\nity that SurVerify outputs REJECTS is\u03b4.\nProof of 5: The proof of this part is simpler than the proof of 5. If dist2\nD\u2217(fS, f\u2217)\u22655\u03f5then\nwe show that SurVerify output ACCEPT in the final ifstatement is less than \u03b4. By Hoeffding\u2019s\ninequality we have the Equation (26). Combining Equation (26) with Lemma 4 and Equation (20)\nwe see that if dist2\nD\u2217(fS, f\u2217)\u22655\u03f5then\nPr[\u02c6\u03b3\u03c4\u2212\u03c4\u02c6LS\u22643\u03c4\u03f5]\u22642\u03b4\n3< \u03b4. (28)\nThis completes the proof.\n24\n\n--- Page 25 ---\nE Lower Bound for Model Reconstruction\nThe task of checking if the regression coefficient for the data in Sis close to the regression coefficient\nforD\u2217can be checked directly by generating an estimate b\u03b8of the optimal regression coefficient\n\u03b8\u2217corresponding to D\u2217. However, the number of samples required for this approximate recovery\nproblem grows with the dimension of the data. The following lemma, due to [DW13] quantifies this\ndependence:\nLemma 19 ( [DW13]) .For a regression model y=\u27e8\u03b8\u2217,x\u27e9+\u03b7with\u03b7\u223c N\u00000, \u03c32\n\u03b7\u0001forx,\u03b8\u2217\u2208Rd\nwithd\u22652, any algorithm that produces an estimate b\u03b8of\u03b8\u2217using msamples must satisfy:\n\n\n\n\u03b8\u2217\u2212b\u03b8\n\n\n2\n2\u22651\n32d2\u03c32\n\u03b7\n\u2225X\u22252\n2,2\nIn particular, If Assumption 2 holds, we have:\n\n\n\n\u03b8\u2217\u2212b\u03b8\n\n\n2\n2\u2265d\u03c32\n\u03b7\n32m\nWe now provide the proof for Lemma 9, restated here.\nLemma 9 (Lower Bound on Testing with Model Reconstruction ).Under Assumption 2, and\nwhen \u03bbmin(Cov ( x))\u2265\u03bbmin, any algorithm that reconstructs the model to estimate the distance\nFDDF\nD\u2217(DS,D\u2217))within \u03f5additive error must make \u2126\u0010d\u03bbmin\u03c32\n\u03b7\n\u03f52\u0011\nqueries.\nProof. Let\u03b8e:=\u03b8\u2217\u2212b\u03b8. Then for any x\u2208Rd, the difference between the true and estimated\npredictions is:\n\u27e8\u03b8\u2217,x\u27e9 \u2212\u00a8b\u03b8,x\u2202\n=\u00a8\n\u03b8\u2217\u2212b\u03b8,x\u2202\n=\u27e8\u03b8e,x\u27e9\nTherefore,\nE\nx\u223cD\u2217\u00ee\n\u27e8\u03b8e,x\u27e92\u00f3\n=E\nx\u223cD\u2217\u00ee\n\u03b8T\nexxT\u03b8e\u00f3\n=\u03b8T\neE\nx\u223cD\u2217\u0002xxT\u0003\u03b8e\n\u2265\u03bbmin\u0010\nE\nx\u223cD\u2217\u0002xxT\u0003\u0011\n\u2225\u03b8e\u22252\n2\n\u2265\u03bbmin\u0010\nE\nx\u223cD\u2217\u0002xxT\u0003\u0011\n\u00b7d\u03c32\n\u03b7\n32mFrom Lemma 19\n=\u03bbmin(Cov ( x))d\u03c32\n\u03b7\n32m\nNow, by setting the distanceq\nEx\u223cD\u2217\u00ee\n\u27e8\u03b8e,x\u27e92\u00f3\nto be less than or equal to \u03f5, we get\nm\u2265\u03bbmind\u03c32\n\u03b7\n32\u03f52.\nHence, we use the loss to identify the model distance between these two quantities. The loss of\nthe two regressions follow two gaussians with different means and same variance. Here, we state a\nlower bound on the difference of means in this setup.\n25\n\n--- Page 26 ---\nF Experimental Results\nIn this section, we detail the outcomes of our experiments described in Section 5. In Table 3 and 4,\nwe list the outcomes of SurVerify on the synthetic dataset w.r.t. the F2andF1model classes,\nrespectively. In Table 5 and 6, we list the outcomes of SurVerify onACS_Income dataset w.r.t.\ntheF2andF1model classes, respectively. As stated in Section 5, we have run 50 trials for all\nparameter choices, i.e. each row in the tables. The \u03b4is set to 0.01throughout. We also reproduce\nthe figures here for ease of reading. The red and blue lines represent the values of the red and blue\nlines of their respective plots as defined in Section 5.\nFigure 5: Acceptance rate of SurVerify w.r.t. model class F2on Synthetic Data vs. change in \u00b5(over 50\nruns) for \u03b4= 0.1and\u03f5= 0.05.\nFigure 6: Acceptance rate of SurVerify w.r.t. model class F1on Synthetic Data vs. change in \u00b5(over 50\nruns) for \u03b4= 0.1and\u03f5= 0.05.\n26\n\n--- Page 27 ---\nTable 3: Performance of SurVerify on Synthetic Data w.r.t. F2[Figure 5]\n\u03f5 FDD Acceptance Rate #Avg. Samples Used \u03c4 Early Rejection Ratio\n0.05 0.04 1 818 818 1\n0.05 0.04 1 818 818 1\n0.05 0.05 1 818 818 1\n0.05 0.06 1 818 818 1\n0.05 0.07 1 818 818 1\n0.05 0.09 1 818 818 1\n0.05 0.11 0.96 818 818 1\n0.05 0.14 0.74 818 818 1\n0.05 0.18 0.06 811 818 0.991\n0.05 0.21 0 758 818 0.927\n0.05 0.25 0 575 818 0.703\n0.05 0.29 0 390 818 0.477\n0.05 0.33 0 277 818 0.339\n0.05 0.39 0 197 818 0.241\n0.05 0.45 0 135 818 0.165\n0.05 0.51 0 100 818 0.122\n0.05 0.57 0 83 818 0.101\n0.05 0.63 0 64 818 0.079\n0.05 0.70 0 52 818 0.063\n0.05 0.78 0 44 818 0.053\n0.05 0.86 0 28 818 0.035\n0.05 0.94 0 29 818 0.035\n0.05 1.02 0 27 818 0.034\n0.05 1.12 0 19 818 0.024\n0.05 1.20 0 18 818 0.022\n0.05 1.31 0 16 818 0.019\n0.05 1.43 0 13 818 0.016\n0.05 1.54 0 10 818 0.013\n0.05 1.62 0 10 818 0.012\n0.05 1.76 0 10 818 0.012\n0.05 1.88 0 9 818 0.011\n27\n\n--- Page 28 ---\nTable 4: Performance of SurVerify on Synthetic Data w.r.t. F1[Figure 6]\n\u03f5 FDD Acceptance Rate #Avg. Samples Used \u03c4 Early Rejection Ratio\n0.05 0.03 1 818 818 1\n0.05 0.04 1 818 818 1\n0.05 0.04 1 818 818 1\n0.05 0.05 1 818 818 1\n0.05 0.07 1 818 818 1\n0.05 0.09 1 818 818 1\n0.05 0.11 1 818 818 1\n0.05 0.13 0.96 818 818 1\n0.05 0.16 0.66 818 818 1\n0.05 0.20 0.1 809 818 0.989\n0.05 0.24 0 726 818 0.888\n0.05 0.28 0 506 818 0.618\n0.05 0.33 0 342 818 0.418\n0.05 0.38 0 225 818 0.276\n0.05 0.43 0 177 818 0.216\n0.05 0.50 0 122 818 0.150\n0.05 0.56 0 89 818 0.109\n0.05 0.62 0 68 818 0.083\n0.05 0.70 0 54 818 0.066\n0.05 0.78 0 51 818 0.063\n0.05 0.85 0 36 818 0.044\n0.05 0.93 0 27 818 0.032\n0.05 1.02 0 21 818 0.026\n0.05 1.10 0 21 818 0.025\n0.05 1.21 0 16 818 0.020\n0.05 1.31 0 14 818 0.017\n0.05 1.43 0 14 818 0.017\n0.05 1.51 0 13 818 0.016\n0.05 1.63 0 11 818 0.013\n0.05 1.75 0 9 818 0.011\n0.05 1.87 0 9 818 0.011\n28\n\n--- Page 29 ---\nFigure 7: Acceptance rate of SurVerify w.r.t. model class F2onACS_Income (over 50 runs) for \u03b4= 0.1\nand varying range of \u03f5.\nFigure 8: Acceptance rate of SurVerify w.r.t. model class F1onACS_Income (over 50 runs) for \u03b4= 0.1\nand varying range of \u03f5.\n29\n\n--- Page 30 ---\nTable 5: Performance of SurVerify onACS_Income w.r.t.F2[Figure 7]\n\u03f5 FDD Acceptance Rate #Avg. Samples Used \u03c4 Early Rejection Ratio\n0.05 0.0258 1 2195 2195 1.000\n0.04 0.0258 0.98 3361 3430 0.980\n0.03 0.0258 0.98 5975 6097 0.980\n0.02 0.0258 1 13717 13717 1.000\n0.0175 0.0258 1 17916 17916 1.000\n0.015 0.0258 0.94 24386 24386 1.000\n0.0125 0.0258 0.52 35115 35115 0.933\n0.01 0.0258 0.12 53222 54867 0.858\n0.009 0.0258 0 57022 67737 0.571\n0.0075 0.0258 0 61388 97542 0.326\n0.006 0.0258 0 52710 152409 0.200\n0.005 0.0258 0 47414 219468 0.136\n0.004 0.0258 0 48322 342919 0.072\n0.003 0.0258 0 45773 609633 0.031\n0.002 0.0258 0 44064 1371675 0.017\n0.0015 0.0258 0 47085 2438532 0.008\n0.001 0.0258 0 44301 5486697 0.007\n0.0005 0.0258 0 42207 21946787 0.002\nTable 6: Performance of SurVerify onACS_Income w.r.t.F1[Figure 8]\n\u03f5 FDD Acceptance Rate #Avg. Samples Used \u03c4 Early Rejection Ratio\n0.05 0.0259 1 2285 2285 1.000\n0.04 0.0259 1 3570 3570 1.000\n0.03 0.0259 0.98 6219 6346 0.980\n0.02 0.0259 0.98 14279 14279 1.000\n0.0175 0.0259 1 18650 18650 1.000\n0.015 0.0259 0.98 25384 25384 1.000\n0.0125 0.0259 0.76 36551 36553 1.000\n0.01 0.0259 0.08 53262 57114 0.933\n0.009 0.0259 0 60467 70511 0.858\n0.0075 0.0259 0 57999 101535 0.571\n0.006 0.0259 0 51660 158649 0.326\n0.005 0.0259 0 45721 228454 0.200\n0.004 0.0259 0 48408 356959 0.136\n0.003 0.0259 0 45663 634593 0.072\n0.002 0.0259 0 43893 1427833 0.031\n0.0015 0.0259 0 42993 2538370 0.017\n0.001 0.0259 0 41880 5711332 0.007\n0.0005 0.0259 0 43961 22845325 0.002\n30",
  "project_dir": "artifacts/projects/SurVerify_Survey_Credibility_Tester",
  "communication_dir": "artifacts/projects/SurVerify_Survey_Credibility_Tester/.agent_comm",
  "assigned_at": "2025-08-31T20:50:05.914328",
  "status": "assigned"
}